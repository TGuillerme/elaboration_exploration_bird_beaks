---
title: "Bird beak's elaboration and exploration"
author: "Thomas Guillerme"
date: "`r Sys.Date()`"
bibliography: references.bib
output:
  html_document:
    fig_width: 8
    fig_height: 8
---

All the custom code and custom functions called in this document can be downloaded and installed in `R` as the standalone working package `beer` (Beak Elaboration and Exploration in R).
Although `beer` is intentionally designed to be portable and shareable, we advise workers to use it with moderation and tailor its consumption to their specific research needs.
Some analyses in `beer` do take some time to run so we advise you do not operate heavy machinery while using it.
Don't use it while driving.
The following code snippets are used to illustrate the implementation of the functions.
See <!-- §§§run_replicates_script --> for running the actual analyses.


```{r install_beer, eval = FALSE}
## Installing beer
devtools::install_github("TGuillerme/elaboration_exploration_bird_beaks/beer")
```

```{r load_beer, eval = TRUE, message = FALSE}
set.seed(42)
## Loading beer
library(mcmcmcglmmm)
library(MCMCglmm)
library(dispRity)

## Loading the demo data and trees
data(demo_data)
data(demo_tree)
traitspace <- demo_data
consensus_tree <- demo_tree
trees_list <- list(demo_tree, demo_tree, demo_tree)
class(trees_list) <- "multiPhylo"
```

# MCMCglmm mini chains analyses

To estimate the variance-covariance matrix for the phylogeny and each clade (see [01-Data_preparation.Rmd](01-Data_preparation.Rmd)) we run multivariate generalised linear mixed models (MCMCglmm) using the `MCMCglmm` package [@MCMCglmm].

## Models


```{r, echo = FALSE}
## Reading the clades list
load("../Data/Processed/levels_list.rda")
```

We ran a general multi-response model on multiple dimensions with one global residual term, a one global random term (phylogeny) and variable clade specific random terms (one per taxonomic level).
We ran these models across five taxonomic levels: the super orders (`r length(levels_list$super_orders)`), the orders (`r length(levels_list$orders)`), the sub-orders (`r length(levels_list$sub_orders)`), the super families (`r length(levels_list$super_families)`) and the families (`r length(levels_list$families)`).
Because of the high number of resulting total random terms (`r sum(unlist(lapply(levels_list, length))) + 1`!) we ran multiple models in an overlapping nested fashion with each taxonomic level including two sub-levels.
For example, one model for all birds including the super order and the order levels as random terms, `r length(levels_list$super_orders)` models at the super order level including the order and the sub order levels as random terms, etc...

Furthermore for each nested model, we only ran models if the taxonomic level contained more than 16 species (two times the number of dimensions, see below) and contained at least two nested sub taxonomic levels with at least 16 species each.

```{r, echo = FALSE}
library(knitr)
model_table <- read.csv("../Data/Processed/models_list.csv")
```

This resulted in using the following `r nrow(model_table)` models:

```{r, echo = FALSE}
kable(model_table)
```

> Note that we do acknowledge that the different taxonomic levels used in this analyses do not have a strong biological justification (i.e. they are just nodes named by biologists) but we found using them nonetheless useful since they refer to commonly known groups in birds (and that alternative choices would be equally arbitrary).

```{r, echo = FALSE}
## Get the number of traits
load("../Data/Processed/shapespace_superfam_lvl_fam.rda")
dimensions <- shapespace_superfam_lvl_fam[[1]]$dimensions
```

Each model was run on `r length(dimensions)` dimensions from the same PCA shapespace (see Data section) <!-- TG:TODO --> using a Gaussian distribution for the fixed effects.
Each model followed this generic syntax:

data = traits + random terms + residuals terms

Or in `MCMCglmm` pseudo code:

```
formula   = PC1:8 ~ trait:clade-1
random    = ~ us(at.level(clade):trait):animal + us(trait):animal
residuals = ~ us(trait):units
```
<!-- TG:TODO: explain that much better) -->

## Mini chains

To increase the speed of the analysis and take phylogenetic uncertainty into account, we used a highly parallelisable "mini chains" approach.
It runs multiple short `MCMCglmm` analyses on multiple trees and pulls the results together into one larger `MCMCglmm` that contains more variation due to phylogenetic uncertainty (similar to `mulTree` @mulTree).
The size of the chains are optimised for speed.


![mini chain diagram](../Manuscript/Figures/mini-chains_diagram.png "Mini chains diagram")


In brief the method works as follows:
 
 1- run three models without burnin on a consensus tree with flat priors: these are the parametrisation chains.
 
 2- extract the burnin length and the posteriors to be used as priors from the parametrisation chains.
 
 3- run multiple models with 10 samplings past the burnin using as priors the posteriors from the parametrisation chains: these are the mini chains.
 
 4- combine all the post burning 10 samples from the mini chains into one big chain to be used as the posterior from the model.

Each step are described and demonstrated in more details below.

### Mini-chains parametrization

We first ran three independent MCMC chains with the model and data described above using the consensus tree and flat priors.
We ran these chains for 20k iterations, sampling every 500 iterations (with no burnin) until the three chains reached an effective sample size >> 200 (§§§and other diagnosis). 
<!-- TG: TODO! -->
 
```{r, parametrise_the_model, eval = FALSE}
## Set up the parametrising chains models on the demo data
param_MCMCglmm <- make.mini.chains(data         = traitspace,
                                   dimensions   = 1:3,
                                   tree         = consensus_tree,
                                   trait.family = "gaussian",
                                   randoms      = c("global", "clade"),
                                   residuals    = "global",
                                   priors       = 0.02,
                                   verbose      = TRUE,
                                   parameters   = list(
                                       nitt   = 50000,
                                       thin   = 500,
                                       burnin = 0))

## Running the three MCMCglmm models
parametrization <- run.mini.chains(param_MCMCglmm, replicates = 3)
```

From these three chains, we first extracted the three burnin periods (defined as the number of iterations when the chain reaches the median likelihood value times 1.1).
We then extracted the posterior estimates of the R-Structure, G-Structure and fixed effects (mean and covariance) (ignoring the runs from the previously estimated burnin periods) to serve as our priors for our mini-chains. 
For those values, we took the highest burnin as the overall burnin and the median of the three posteriors as the overall priors.

```{r, get_model_parameters, eval = FALSE}
## Extract the parameters from the parameter chains
estimated_params <- extract.parameters(parametrization)
```

### Running the mini-chains

We set up a mini-chain to be a MCMCglmm running with:
  1) the model described above,
  2) a random tree, and
  3) the estimated priors (with a degree of parameter belief of 5% `nu = 0.05`) to run for `nitt` generations were `nitt` is equal to the previously estimated burnin phase + 10 sampled iterations (`nitt = burnin + thin * 10`).
Each mini-chain thus provided us with 10 posterior exploitable data points.
The two main advantages of this mini-chain approach is that
  1) they are much faster to run since no diagnosis of convergence is necessary and the chains are only run for a relatively short time (which allow several chains to crash/fail without losing all the outputs); and
  2) they take into account tree uncertainty without having to run the complete `MCMCglmm` on all trees (c.f. `mulTree` @mulTree).


```{r, set_the_mini_chains, eval = FALSE}

## The thinning parameter
thin <- 500
## The number of samples per chains
samples <- 10

## Set up the mini-chains models
parametrisation_chain <- make.mini.chains(
                                data         = traitspace,
                                dimensions   = 1:3,
                                tree         = trees_list,
                                trait.family = "gaussian",
                                randoms      = c("global", "clade"),
                                residuals    = "global",
                                priors       = est_params$priors,
                                verbose      = TRUE,
                                parameters   = list(
                                    nitt   = est_params$burnin + thin * samples,
                                    thin   = thin,
                                    burnin = est_params$burnin))
```

We then run the `MCMCglmm` over 100 randomly selected trees to get an estimate of the variance-covariance matrices including phylogenetic uncertainty.

```{r, running_replicates, eval = FALSE}
## The number of replicates
n_reps <- 100

## Run the required chains
results <- run.mini.chains(mini_chains, replicates = n_reps)

## Combining the results as one big MCMC
combined_results <- combine.mini.chains(results)
```



<!-- 

#### Number of runs optimisation (TO BE DEVELOPED?)

```{r, eval = FALSE}
## Function for running one mini chain on a random tree (and saving the results)
fun.mini.chains <- function() {
    run.mini.chains(mini_chains, replicates = 1, path = "../Data/Processed/MCMC/", file.name = "mini.chains")
}
```

To make sure we reached consistent results, we run the set of N mini-chains were N was determined using the following variance optimisation algorithm.
 
 * 1. run an initial $N = 20$ chains;
 * 2. estimate the $V_{1}$, the pooled variances for each parameters in the chain (V1 is a vector of length §§§);
 * 3. update N to N = N * 1.05 and run an additional 5% chains;
 * 4. estimate $V_{2}$, the pooled variances for each parameters in the chain;
 * 5. calculate $\Delta_V$, the absolute change between $V_{1}$ and $V_{2}$ as ($\sqrt{(\frac{V_{1}}/{V_{2}} - 1)^{2}}$).
 * 6. If $\Delta_V < 0.05$ (meaning an increase of 5% of the replicates resulted in an increase of the parameters of less than 5%), the desired number of mini-chains is reached. Else, set $V_{1} = V_{2}$ and repeat from step 3.


```{r, optimised_replicates, eval = FALSE}
## The summary function

```


After reaching the desired $N$ chains, we pulled all the posterior estimates together back into a "standard" big MCMCglmm with a posterior distribution including the effective results and the phylogenetic uncertainty.

-->

## References
