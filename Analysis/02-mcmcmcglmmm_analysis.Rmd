---
title: "Projection analysis to measure biological elaboration and innovation."
author: "Thomas Guillerme, Natalie Cooper, Andrew P Beckerman,  Gavin H Thomas"
bibliography: references.bib
date: "`r Sys.Date()`"
output:
  html_document:
    fig_width: 8
    fig_height: 8
    keep_tex: true
  self_contained: true
---

```{r header, echo = FALSE, results = 'hide', message = FALSE, warning = FALSE, eval = TRUE}
library(mcmcmcglmmm)
library(dispRity)
``` 


# Mini chains MCMCglmm method: an efficient way to calculate variance-covariance matrices on big data with phylogenetic uncertainty

One commonly used method to estimate variance-covariance matrices from phylogenetic datasets is to use multivariate generalised linear mixed models.
This is often done in a Bayesian way using the `MCMCglmm` package (@hadfield2010; cited nearly 4000 times in the last decade).
The MCMCglmm method allows to run a generalised linear mixed model on a multidimensional dataset with an underlying phylogenetic structure.
<!-- Expand that from the longevity paper -->.

Using this method we can run a nested phylogenetic model on a multidimensional dataset (here the Charadriiformes PCA shapespace).
This model will have a the multidimensional trait values as a response and the classic residual terms as an error term.
Additionally though, we can add a random error term as the phylogenetic signal.
This random error term can be expanded into four independent nested terms: one for the entire phylogenetic structure and three for each individual clade's phylogenetic structure.

$$\text{data} = \text{traits} + \text{residuals} + (\text{phylogenetic term} + \text{clade term})$$

Or in `MCMCglmm` pseudo code:

```
formula   = PC1:8 ~ trait:clade-1
random    = ~ us(at.level(clade):trait):animal + us(trait):animal
residuals = ~ us(trait):units
```
<!-- TG:TODO: explain that much better) -->

Although this method is perfectly efficiently implemented on small to relatively big datasets, the major hurdle is the calculation of the variance-covariance matrix that increases following a power law with both the number of species and the number of dimensions.
Therefore, such method becomes easily untractable on large datasets (>8k species and >3 dimensions).
Furthermore, this method's phylogenetic correction is based on a single input tree (usually the consensus tree).
This can also be problematic on big trees (i.e. > 5k species) where the notion of a consensus tree being a good representation of phylogenetic is at best misleading and at worth incorrect.
In fact, the bigger the tree, the more there is variance in topological relation between species (especially in terms of branch length towards deep branches).
Therefore, there has been a push to use tree distributions rather than consensus tree (i.e. point estimates) in using the MCMCglmm method (@healy2014; @mulTree).
This however, leads to yet another untractable computer problem: it requires running the large dataset much more than once on multiple trees (e.g. if using @mulTree) thus linearly increasing computational time.

Thus, to increase the speed of these analyses, while taking phylogenetic uncertainty into account, we used a highly parallelisable "mini chains" approach.
In brief, it runs multiple short `MCMCglmm` analyses on multiple trees and pulls the results together into one larger `MCMCglmm` that contains more variation due to phylogenetic uncertainty (where the size of the chains are optimised for speed and low RAM usage).

## Mini chains

The mini chains `MCMCglmm` method (`mcmcmcglmmm`) effectively works by running many very short parallel `MCMCglmm` on as many different tree topologies.
Effectively each mini chain generates a very small number of samples post-burnin (e.g. 10) and combine them across many replicates (e.g. 1000) into a chain containing effectively many samples (e.g. 10000), each ran independently on a different tree toplogy.

![Figure 2](../Figures/mini-chains_diagram.png)

##### Figure 2: {#fig2}
Mini chains diagram: the numerical values displayed here (number of chains, priors, etc...) are used as examples and can be tuned to fit specific questions and datasets.

In more details, the method works as follows:

 1. Running a small number of "parametrisation chains": these chains are run with the traits data and the model of interest on a consensus tree from the tree distribution, along with flat priors with a low belief parameter (effectively not putting any important weight on the priors). 
 2. Extracting parameters from the "parametrisation chains": this step allows to extract the parameters of interest from the posteriors of the "parametrisation chains". The parameters of interest are the conservative burnin average time (here defined as highest number of iterations required to reach the median posterior likelihood across the parametrisation chains with an additional 10% extra generations) and the mini chains priors (the median posterior model results - ignoring the previously estimated burnin - with a slightly higher belief parameter than for the previous chains).
 3. Running the mini chains: these chains use the same data and model as for the "parametrisation chains" but use a random tree from the tree distribution rather than the consensus one. Furthermore, they use the parameters previously estimated (i.e. the priors and the burnin phase). These chains are run just for a small number of itterations past the burnin phase to extract a fixed number of samples (e.g. if the sampling rate is every 100 generations and the requested number of samples, the chain runs for $\text{burnin iterations} + \text{requested_samples} \times \text{sampling rate}$).
 4. Combining the mini chains: finally we can combine the post-burnin samples from all the mini chains back into a single classical `MCMCglmm` chain (with no burnin).

Some note about diagnosis.

### Implementation

The mini chain approach is implemented on the github package `mcmcmcglmmm` (link).
The package comes with a demo vignette explaining in details how to reproduce each step in the procedure described above.

### Effectiveness

In a practical implication of the mini-chains, we ran for another project a similar model on 8748 species with 8 dimensions and 31 nested levels.
Using this method, it took effectively 20 parallel cores with 8GB of RAM 2 months to complete the model with each individual mini-chain running for around 40 hours (using the SHARC cluster from Sheffield: CITE).
We estimated that the same model using the normal `MCMCglmm` implementation would have required 2.1 years and 4TB of RAM. 

This method thus allows to run multivariate generalised linear mixed effects models on large datasets and across a tree distribution.
The posteriors of this analyses contain a number of variance-covariance matrices (`model$VCV` in a `MCMCglmm` output) and their location in the trait space (`model$Sol` in a `MCMCglmm` output).
We can then use these distributions of variance-covariance matrices as major axes in the multidimensional space on which to run the elaboration and exploration analyses.

# Reproducible example

Here we illustrate the BEER pipeline by applying it to a data set from @cooney2017 focused on the birds charadriiformes order.

## Data

This order contains 359 species divided into three clades: gulls, sandpipers and plovers and sandpipers that have respectively 159, 102 and 98 species each.
For each of these species, we used the 3D beak dataset from @cooney2017 that is an ordination of the shapes of the beaks of 8@@@ birds (the resulting shape space used here does not contain any information about the beak size - i.e. centroid size).
We extracted the 359 charadriiformes species from this space where the first three dimensions account for more than 99% of the variance in the dataset.
For the rest of the example analysis here, we used as a traitspace of these 359 species with 3 dimensions (hereafter the shapespace).

```{r, echo = FALSE, eval = TRUE}
## Loading the charadriiformes data
data(charadriiformes) 
## Extracting the tree
tree <- charadriiformes$tree
## Extracting the data column that contains the clade assignments
data <- charadriiformes$data[, "clade"]
## Changing the levels names (the clade names) to colours
levels(data) <- c("orange", "blue", "darkgreen")
data <- as.character(data)
## Matching the data rownames to the tip order in the tree
data <- data[match(ladderize(tree)$tip.label, rownames(charadriiformes$data))]

## Matching the tip colours (labels) to their descending edges in the tree
## (and making the non-match edges grey)
clade_edges <- match.tip.edge(data, tree, replace.na = "grey")

## Plotting the results
plot(ladderize(tree), show.tip.label = FALSE, edge.color = clade_edges, main = "Charadriiformes")
legend("bottomleft", lty = c(1,1,1), col = c("blue", "darkgreen", "orange"), legend = c("plovers", "sandpipers", "gulls"))
axisPhylo()
```

##### Figure 5: {#fig5}
The Charadriiformes data used in this pipeline illustration. The axis units are millions of years ago.


## Mini-chains MCMCglmm method

To illustrate the BEER pipeline, we analysed the variance-covariance of the shapespace using a generalised linear mixed model with two different levels of random terms: one for the whole charadriiformes phylogeny and one different one for each different clade (gulls, plovers and sandpipers).
We first run three parametrisation chains for 50000 generations with no burnin and a sampling every 500 generations.
From this chain we extracted: 1) the maximum burnin time as 1.1 times the number of generations required to reach the median likelihood value and 2) the median posteriors of the model (discarding the burnin) as priors for our mini chains models with a believe parameter value of 5%.
We then ran 50 minichains for 10 samples each using the previous sampling rate (500 generations) and the extracted parameters form the parametrisation chain (the burnin and priors described above).

```{r, echo = FALSE, eval = FALSE}
library(mcmcmcglmmm)

## Making the parametrisation chains
parametrisation_chains <- make.mini.chains(data = charadriiformes$data,
                                           dimensions = c("PC1", "PC2", "PC3"),
                                           randoms = c("global", "clade"),
                                           residuals = "global",
                                           tree = charadriiformes$tree,
                                           parameters = list(nitt = 50000, burnin = 0, thin = 500))

## Running the parametrisation chains
param_results <- run.mini.chains(parametrisation_chains, replicates = 3)
save(param_results, file = "param_results.rda")
load("param_results.rda")

diagnosis <- diagnose.mini.chains(param_results)
## Extracting the parameters
params <- extract.parameters(param_results)

## The sampling rate:
sampling <- 500
## The required number of samples per tree
samples <- 10

## Making the mini chains
parametrisation_chains <- make.mini.chains(data = charadriiformes$data,
                                           dimensions = c("PC1", "PC2", "PC3"),
                                           randoms = c("global", "clade"),
                                           residuals = "global",
                                           priors = params$priors,
                                           tree = charadriiformes$tree_distribution,
                                           parameters = list(
                                            nitt = params$burnin + samples*sampling,
                                            burning = params$burnin,
                                            thin = sampling))
## Running the mini chains
results <- run.mini.chains(parametrisation_chains, replicates = 100)
posteriors <- combine.mini.chains(results)
save(posteriors, file = "posteriors.rda")
```

This resulted in obtaining 1000 exploitable posterior variance-covariance matrices for each residual terms of our model: the overall phylogenetic effect and the three specific phylogenetic effects for each of the three clades.


```{r, echo = FALSE, eval = TRUE}
load("posteriors.rda")
silent <- diagnose.mini.chains(posteriors, plot = TRUE)
```

<!-- Do some developments about this part -->

