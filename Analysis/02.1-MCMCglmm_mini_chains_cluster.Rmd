---
title: "Bird beak's elaboration and exploration"
author: "Thomas Guillerme"
date: "`r Sys.Date()`"
bibliography: references.bib
output:
  html_document:
    fig_width: 8
    fig_height: 8
---

This script is a working script to run the chains on the SHARC Sheffield cluster.
Refer to `02-MCMCglmm_mini_chains.Rmd` for more details and reproducibility

```{r install_beer, eval = FALSE}
## Installing beer
devtools::install_github("TGuillerme/elaboration_exploration_bird_beaks/beer")
```

```{r load_beer, eval = TRUE, message = FALSE}
set.seed(42)
## Loading beer
library(mcmcmcglmmm)
library(MCMCglmm)
library(dispRity)

## Loading the parametrisation data
load(file = "../Data/Processed/form_shape_space_1tree_2levels.rda")

## Select the data
shapespace <- form_shape_space_1tree_2levels$shape_space
groups_list <- form_shape_space_1tree_2levels$groups_list
selected_dim <- form_shape_space_1tree_2levels$shape_dim
consensus_tree <- form_shape_space_1tree_2levels$consensus_tree

```

# All birds shapespace analyses

## Mini-chains parametrization

```{r, parametrise_the_model, eval = FALSE}
## Set up the parametrising chains models
param_MCMCglmm <- make.mini.chains(data         = shapespace,
                                   dimensions   = selected_dim,
                                   tree         = consensus_tree,
                                   trait.family = "gaussian",
                                   randoms      = c("global", "level1", "level2"),
                                   residuals    = "global",
                                   priors       = 0.02,
                                   verbose      = TRUE,
                                   parameters   = list(
                                       nitt   = 20000,
                                       thin   = 500,
                                       burnin = 0))

## Running the three MCMCglmm models
100k_params_shapespace_8D_31levels <- run.mini.chains(param_MCMCglmm, replicates = 1)
save(param_chains, file = "../Data/Processed/MCMCglmm/100k_params_shapespace_8D_31levels_chain1.rda")
```

From these three chains, we first extracted the three burnin periods (defined as the generation when the chain reaches the median likelihood value times 1.25).
We then extracted the posterior estimates of the R-Structure, G-Structure and fixed effects (mean and covariance) (ignoring the runs from the previously estimated burnin periods) to serve as our priors for our mini-chains. 
For those values, we took the highest burnin as the overall burnin and the median of the three posteriors as the overall priors.

```{r, get_model_parameters, eval = FALSE}
## Load the computed models
param_chains <- list()
for(i in 1:3) {
    load(paste0("../Data/Cluster/allbirds_8748t_8Dshapespace_31levels/param_chains/params_shapespace_8D_31levels_20k_chain", i, ".rda"))
    param_chains[[i]] <- params_shapespace_8D_31levels[[1]]
}
## Extract the parameters from the parameter chains
est_params <- extract.parameters(param_chains)
```

### Running the mini-chains

```{r, set_the_mini_chains, eval = TRUE}
## Get the distribution of trees
load(file = "../Data/Processed/trees_list.rda")

## The thinning parameter
thin <- 200
## The number of samples per chains
samples <- 10

## Set up the mini-chains models
shapespace_8748t_8D_32level <- make.mini.chains(
                                data         = shapespace,
                                dimensions   = selected_dim,
                                tree         = trees_list[sample(1:1000, 20)],
                                trait.family = "gaussian",
                                randoms      = c("global", "level1", "level2"),
                                residuals    = "global",
                                priors       = est_params$priors,
                                verbose      = TRUE,
                                parameters   = list(
                                    nitt   = est_params$burnin + thin * samples,
                                    thin   = thin,
                                    burnin = est_params$burnin))
## Save the mini.chains
save(shapespace_8748t_8D_32level, file = "../Data/Cluster/allbirds_8748t_8Dshapespace_31levels/shapespace_8748t_8D_32level.mini.chains")
```

The chains are then run on the cluster
TODO: details


### Combining the model data

```{r}
## Preparing the model files
model_list <- list()
class(model_list) <- c("beer", "mini.chains")
models_path <- "../Data/Cluster/allbirds_8748t_8Dshapespace_31levels/cluster_out/"
## Loading all models
for(file in list.files(models_path)) {
    load(paste0(models_path, file, collapse = "/"))
    model_list[[length(model_list)+1]] <- model[[1]]
}

## Combining the model files
allbirds_8748t_8Dshapespace_31levels <- combine.mini.chains(model_list)

## Saving the results
save(allbirds_8748t_8Dshapespace_31levels, file = "../Data/processed/allbirds_8748t_8Dshapespace_31levels.rda")
```

<!-- 

#### Number of runs optimisation (TO BE DEVELOPED?)

```{r, eval = FALSE}
## Function for running one mini chain on a random tree (and saving the results)
fun.mini.chains <- function() {
    run.mini.chains(mini_chains, replicates = 1, path = "../Data/Processed/MCMC/", file.name = "mini.chains")
}
```

To make sure we reached consistent results, we run the set of N mini-chains were N was determined using the following variance optimisation algorithm.
 
 * 1. run an initial $N = 20$ chains;
 * 2. estimate the $V_{1}$, the pooled variances for each parameters in the chain (V1 is a vector of length §§§);
 * 3. update N to N = N * 1.05 and run an additional 5% chains;
 * 4. estimate $V_{2}$, the pooled variances for each parameters in the chain;
 * 5. calculate $\Delta_V$, the absolute change between $V_{1}$ and $V_{2}$ as ($\sqrt{(\frac{V_{1}}/{V_{2}} - 1)^{2}}$).
 * 6. If $\Delta_V < 0.05$ (meaning an increase of 5% of the replicates resulted in an increase of the parameters of less than 5%), the desired number of mini-chains is reached. Else, set $V_{1} = V_{2}$ and repeat from step 3.


```{r, optimised_replicates, eval = FALSE}
## The summary function

```


After reaching the desired $N$ chains, we pulled all the posterior estimates together back into a "standard" big MCMCglmm with a posterior distribution including the effective results and the phylogenetic uncertainty.

-->

## References
